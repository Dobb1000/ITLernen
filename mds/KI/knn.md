

# ğŸ“Œ KNN Algorithmus (K-Nearest Neighbors)

## Was ist der KNN-Algorithmus?

- **KNN = K-Nearest Neighbors (K-nÃ¤chste Nachbarn)**
- **UnÃ¼berwachtes oder Ã¼berwacht lernendes Verfahren** â†’ Meist genutzt fÃ¼r **Klassifikation**, manchmal auch fÃ¼r **Regression**.
- GehÃ¶rt zu den **Lazy-Learnern** â†’ Kein explizites Trainingsmodell wird erstellt, alle Berechnungen finden bei der Vorhersage statt.
- Idee: **Ein Datenpunkt wird klassifiziert basierend auf den Klassen der K nÃ¤chsten Nachbarn** (nach Distanz gemessen).

---

## Bedeutung von **K**

- **K = Anzahl der nÃ¤chsten Nachbarn**, die bei der Klassifikation berÃ¼cksichtigt werden.
- Beispiel:
  - **K = 3** â†’ Die 3 nÃ¤chsten Punkte werden angeschaut.
  - Die Klasse, die am hÃ¤ufigsten unter diesen Nachbarn vorkommt â†’ wird als Vorhersage genommen.

### **Einfluss der K-Wahl:**

| K-Wert | Bedeutung | Auswirkungen |
|-------|----------|--------------|
| **K = 1** | Nur der nÃ¤chste Nachbar zÃ¤hlt. | Sehr empfindlich gegenÃ¼ber AusreiÃŸern. Kann zu Overfitting fÃ¼hren. |
| **Niedriges K (klein)** | Wenige Nachbarn berÃ¼cksichtigt. | Sehr flexibel, kann aber stark auf Rauschen reagieren. |
| **Hohes K (groÃŸ)** | Viele Nachbarn berÃ¼cksichtigt. | GlÃ¤ttet Entscheidungen, aber kann wichtige Details verlieren â†’ Risiko von Underfitting. |
| **Optimal: HÃ¤ufig ungerade Zahl** (bei Klassifikation) | Verhindert Gleichstand. |  |

â†’ **Tipp:** K oft per **Cross-Validation** wÃ¤hlen!

---

## Distanzverfahren (Distance Metrics)

Die Distanz zwischen Punkten ist entscheidend fÃ¼r KNN! Typische Verfahren:

<img src="/ITLernen/tutorial/KI/img/distanzberechnung.svg" style="width: 70%">

Distanzfunktionen fÃ¼r $\mathbf{P} = (p_1 \mid \dots \mid p_n)$ und $\mathbf{Q} = (q_1 \mid \dots \mid q_n)$}
### Euklidische Distanz
$d(p, q) = \sqrt{ (p_1 - q_1)^2 +  (p_2 - q_2)^2 }$

### Manhattan Distanz
$d(P, Q) = \sum_{i=1}^{n} \left| p_i - q_i \right|
$

### Maximum-Distanz (Chebyshev-Distanz)
$d(P, Q) = \max \left( \left| p_i - q_i \right| \right)$


## Weitere wichtige Punkte

### âœ… **Vorteile:**

- Einfach & intuitiv.
- Keine Annahmen Ã¼ber Datenverteilung.
- Gut fÃ¼r **kleine bis mittlere DatensÃ¤tze**.

### âŒ **Nachteile:**

- **Rechenintensiv** bei groÃŸen DatensÃ¤tzen â†’ Alle Distanzen mÃ¼ssen berechnet werden.
- **Speicherintensiv**, da alle Datenpunkte gespeichert werden.
- Sensibel gegenÃ¼ber irrelevanten Features â†’ **Feature Scaling** oft notwendig (z.B. Standardisierung).
- Schlechter bei **hoher DimensionalitÃ¤t** (Curse of Dimensionality).

---

## Typische Anwendungen:

- **Mustererkennung** (Bilder, Handschriften).
- **Empfehlungssysteme**.
- **Medizinische Diagnosen**.
- **Textklassifikation**.

# Beipsiel KNN-Visualisierung

<iframe style="height: 150vh" onload="resizeIframe(this)" src="/ITLernen/canvas/KI/KNN/knnvis.html" id="not"></iframe>
