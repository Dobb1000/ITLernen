

# üìå KNN Algorithmus (K-Nearest Neighbors)

## Was ist der KNN-Algorithmus?

- **KNN = K-Nearest Neighbors (K-n√§chste Nachbarn)**
- **Un√ºberwachtes oder √ºberwacht lernendes Verfahren** ‚Üí Meist genutzt f√ºr **Klassifikation**, manchmal auch f√ºr **Regression**.
- Geh√∂rt zu den **Lazy-Learnern** ‚Üí Kein explizites Trainingsmodell wird erstellt, alle Berechnungen finden bei der Vorhersage statt.
- Idee: **Ein Datenpunkt wird klassifiziert basierend auf den Klassen der K n√§chsten Nachbarn** (nach Distanz gemessen).

---

## Bedeutung von **K**

- **K = Anzahl der n√§chsten Nachbarn**, die bei der Klassifikation ber√ºcksichtigt werden.
- Beispiel:
  - **K = 3** ‚Üí Die 3 n√§chsten Punkte werden angeschaut.
  - Die Klasse, die am h√§ufigsten unter diesen Nachbarn vorkommt ‚Üí wird als Vorhersage genommen.

### **Einfluss der K-Wahl:**

| K-Wert | Bedeutung | Auswirkungen |
|-------|----------|--------------|
| **K = 1** | Nur der n√§chste Nachbar z√§hlt. | Sehr empfindlich gegen√ºber Ausrei√üern. Kann zu Overfitting f√ºhren. |
| **Niedriges K (klein)** | Wenige Nachbarn ber√ºcksichtigt. | Sehr flexibel, kann aber stark auf Rauschen reagieren. |
| **Hohes K (gro√ü)** | Viele Nachbarn ber√ºcksichtigt. | Gl√§ttet Entscheidungen, aber kann wichtige Details verlieren ‚Üí Risiko von Underfitting. |
| **Optimal: H√§ufig ungerade Zahl** (bei Klassifikation) | Verhindert Gleichstand. |  |

‚Üí **Tipp:** K oft per **Cross-Validation** w√§hlen!

---

## Distanzverfahren (Distance Metrics)

Die Distanz zwischen Punkten ist entscheidend f√ºr KNN! Typische Verfahren:

<img src="/ITLernen/tutorial/KI/img/distanzberechnung.svg" style="width: 70%">

Distanzfunktionen f√ºr $\mathbf{P} = (p_1 \mid \dots \mid p_n)$ und $\mathbf{Q} = (q_1 \mid \dots \mid q_n)$}
### Euklidische Distanz
$d(p, q) = \sqrt{ (p_1 - q_1)^2 +  (p_2 - q_2)^2 }$

### Manhattan Distanz
$d(P, Q) = \sum_{i=1}^{n} \left| p_i - q_i \right|
$

### Maximum-Distanz (Chebyshev-Distanz)
$d(P, Q) = \max \left( \left| p_i - q_i \right| \right)$

#### **Formel:**

F√ºr zwei Punkte
- **$P‚ÇÅ = (x‚ÇÅ, y‚ÇÅ)$**
- **$P‚ÇÇ = (x‚ÇÇ, y‚ÇÇ)$**

ist die **Chebyshev-Distanz:**

$
D = \max\left( |x‚ÇÅ - x‚ÇÇ| , |y‚ÇÅ - y‚ÇÇ| \right)
$

Also:  
‚û°Ô∏è Berechne den **Unterschied in x** und **Unterschied in y** (als Betrag),  
‚û°Ô∏è und nimm davon den **gr√∂√üeren Wert**!

---

#### **Beispiel:**

Punkt A = (3, 5)  
Punkt B = (7, 2)

Rechnung:

$
|3 - 7| = 4
$

$
|5 - 2| = 3
$

Jetzt:  
$
D = \max(4, 3) = 4
$

**Die Chebyshev-Distanz ist 4.**



## Weitere wichtige Punkte

### ‚úÖ **Vorteile:**

- Einfach & intuitiv.
- Keine Annahmen √ºber Datenverteilung.
- Gut f√ºr **kleine bis mittlere Datens√§tze**.

### ‚ùå **Nachteile:**

- **Rechenintensiv** bei gro√üen Datens√§tzen ‚Üí Alle Distanzen m√ºssen berechnet werden.
- **Speicherintensiv**, da alle Datenpunkte gespeichert werden.
- Sensibel gegen√ºber irrelevanten Features ‚Üí **Feature Scaling** oft notwendig (z.B. Standardisierung).
- Schlechter bei **hoher Dimensionalit√§t** (Curse of Dimensionality).

---

## Typische Anwendungen:

- **Mustererkennung** (Bilder, Handschriften).
- **Empfehlungssysteme**.
- **Medizinische Diagnosen**.
- **Textklassifikation**.

# Beipsiel KNN-Visualisierung

<iframe style="height: 150vh" onload="resizeIframe(this)" src="/ITLernen/canvas/KI/KNN/knnvis.html" id="not"></iframe>
